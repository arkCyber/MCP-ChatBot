# AI Server Configuration

# Server Configuration
server:
  base_url: "http://localhost:11434"  # Ollama default URL
  timeout: -1  # 无限等待
  max_retries: 3  # 最大重试次数

# API Keys
api_keys:
  openai: "not-used"
  anthropic: "not-used"
  azure: "not-used"
  deepseek: "not-used"

# Model Configuration
models:
  default: "llama2"
  available:
    - name: "llama2"
      max_tokens: 8192
      temperature: 0.7
    - name: "codellama"
      max_tokens: 4096
      temperature: 0.7
  ollama: "llama3.2:latest"
  openai: "gpt-4"
  deepseek: "deepseek-chat"

# Rate Limiting
rate_limits:
  requests_per_minute: 60
  tokens_per_minute: 90000

# Logging Configuration
logging:
  level: "INFO"
  file: "logs/app.log"
  max_size: 100
  max_backups: 5

# Cache Configuration
cache:
  enabled: true
  ttl: 3600  # 缓存生存时间（秒）
  max_size: 1000  # 最大缓存条目数 

# Vector Database Configuration
vector_db:
  provider: "qdrant"
  host: "localhost"
  port: 6333
  http_port: 6334
  collection_name: "obsidian_docs"
  vector_size: 384  # Sentence-Transformers all-MiniLM-L6-v2 模型输出维度
  distance: "Cosine"
  on_disk_payload: true
  optimizers_config:
    default_segment_number: 2
    memmap_threshold: 20000
  wal_config:
    wal_capacity_mb: 32
    wal_segments_capacity_mb: 64
  performance_config:
    max_search_threads: 8  # M1 多核心优化
    max_optimization_threads: 4  # M1 多核心优化

# 存储路径配置
storage:
  raw_docs_path: "data/raw_docs"
  vectors_path: "data/vectors"
  temp_path: "data/temp"

# Obsidian 配置
obsidian:
  vault_path: "~/Documents/Obsidian"  # 请替换为您的 Obsidian vault 路径
  file_extensions: [".md", ".txt"]
  exclude_patterns: ["node_modules", ".git"]

# 向量化配置
embedding:
  model: "all-MiniLM-L6-v2"  # 使用的 Sentence-Transformers 模型
  batch_size: 64  # M1 内存较大，可以增加批处理大小
  device: "mps"  # 使用 M1 的 Metal Performance Shaders
  cache_dir: "models"  # 模型缓存目录
  num_threads: 8  # M1 多核心优化
  memory_limit: "8GB"  # 内存限制
  optimization:
    use_mps: true  # 启用 MPS 加速
    use_metal: true  # 启用 Metal 加速
    use_parallel: true  # 启用并行处理
    chunk_size: 1024  # 文本分块大小 